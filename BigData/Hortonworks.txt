## Hortonworks

Website: https://hortonworks.com/

<something-something> is inchortapptest01.inceptum.local

## Administration before tutorials

### Sandbox Deployment and Install Guide

https://hortonworks.com/tutorial/sandbox-deployment-and-install-guide/

Can be deployed in multiple environments.

HDP (Data-At-Rest): data in a database or distributed using HDFS (Hadoop Distributed File System). Send queries to data not data to queries (as you would do normally).

#### Hortonworks Sandbox Guide

https://hortonworks.com/tutorial/hortonworks-sandbox-guide/

System info and which services are running and which are stopped.

Ports for portfowarding.

### Sandbox Port Forwarding Guide

https://hortonworks.com/tutorial/sandbox-port-forwarding-guide/

Each VM has its own guide.





Tutorials: https://hortonworks.com/tutorials/

## Hadoop Tutorial – Getting Started with HDP

### Learning the Ropes of the HDP Sandbox

https://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox/

Sandbox VM is installed.
Attached to a virtual network.
By default, NAT (Network Address Translation) network mode.

NAT: guest IP == host IP, external devices cannot conntect to the guest (the Vritual Machine), but the guest can connect to external devices.
     can portfoward to make them reachable.

Bridged Networking: guest can be reached and can reach external devices.
                    needed for Connected Data Architecture.

Determine IP of Sandbox: ??

Terminal access: through Putty <something-something> or Web GUI <something-something>:4200

Tranfser data: scp -P 2222 <from> <to>
Example: scp -P 2222 <local_directory_file> root@sandbox-hdp.hortonworks.com:<sandbox_directory_file>
OR
Example: scp -P 2222 root@sandbox-hdp.hortonworks.com:<sandbox_directory_file> <local_directory_file>

Sandbox Welcome Page: http://sandbox-hdp.hortonworks.com:8888/ ???

Explore Ambari:
<something-something>:8080
                
## Hadoop Tutorial – Getting Started with HDP

https://hortonworks.com/tutorial/hadoop-tutorial-getting-started-with-hdp/

### Concepts

HDP (Hortonworks Data Platform)
Hadoop Distributed File System for data storage
YARN is the Data Operating System that allows Hive/Pig and other languages to access the data

Apache Hadoop distribute and store data.
Made of Hadoop Common, HDFS, YARN, MapReduce.
HDP manages Hadoop clusters.

HDFS partitions data across multiple computers. A single computer cannot hold all the data.
HDFS Cluster: NameNode - manage cluster matedata (replicas, data block mapping, requests, ...)
              CataNode - sotre data

Ambari File USer View: next to login credentials, in a dropdowwn box, "File View"

MapReduce: algorithm to distribute and compute data TODO
YARN: generic resource manager TODO

Hive: SQL database for big datasets TODO
Tez: batch and interactive data processing, improves MapReduce TODO
Stinger: TODO
Ambari Hive View: next to login credentials, in a dropdowwn box, "Hive View 2.0" TODO

Pig: simplify MapReduce scripts TODO
Ambari Pig View: next to login credentials, in a dropdowwn box, "Pig View" TODO

### Loading Sensor Data into HDFS

Open Files View.
Goto /user/<someone>, for example /user/maria_dev.
Create a new folder and upload the files there.
Select folder and change Write permissions to solid BLUE.

### Hive - Data ETL

Open Hive View 2.0.

Query: for SQl
Jobs: query histroy
Tables: view tables
Saved queries: self explanatory
UDFs: uder defined functions
Settings: modify Hive

Create table, upload table, tell it where you can find some data, click preview.
Example: there is a header, from HDFS...

You can write queries in the Query Tab.
TEZ View: TODO

Possible to create a table from SELECT. CTAS (Create Table As Select).

### Pig - Risk Factor

Open Pig View.
Create a Pig script with the .pig extension.
Use Pig Latin: http://pig.apache.org/docs/r0.14.0/basic.html

'''
a = LOAD 'geolocation' using org.apache.hive.hcatalog.pig.HCatLoader();
b = filter a by event != 'normal';
c = foreach b generate driverid, event, (int) '1' as occurance;
d = group c by driverid;
e = foreach d generate group as driverid, SUM(c.occurance) as t_occ;
g = LOAD 'drivermileage' using org.apache.hive.hcatalog.pig.HCatLoader();
h = join e by driverid, g by driverid;
final_data = foreach h generate $0 as driverid, $1 as events, $3 as totmiles, (float) $3/$1 as riskfactor;
store final_data into 'riskfactor' using org.apache.hive.hcatalog.pig.HCatStorer();
'''

'''
The line a = loads the geolocation table from HCatalog.
The line b = filters out all the rows where the event is not ‘Normal’.
Then we add a column called occurrence and assign it a value of 1.
We then group the records by driverid and sum up the occurrences for each driver.
At this point we need the miles driven by each driver, so we load the table we created using Hive.
To get our final result, we join by the driverid the count of events in e with the mileage data in g.
Now it is real simple to calculate the risk factor by dividing the miles driven by the number of events
Note: add -useHCatalog to the argument box; that way Pig will load the proper libraries
'''

### Spark - Risk Factor

RDD (Resilient Distributed Dataset)

View the Zeppelin Notebook page: <something-something>:9995/

%jdbc(hive) show tables // -> show tables

%spark2
val hiveContext = new org.apache.spark.sql.SparkSession.Builder().getOrCreate() // -> instantiate sparksession

%spark2
hiveContext.sql("show tables").show() // -> show tables ready to be queried

%spark2data 
val geolocation_temp1 = hiveContext.sql("select * from geolocation") // -> transfer data from table to Spark RDD
