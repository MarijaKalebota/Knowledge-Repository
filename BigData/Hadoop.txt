## Hadoop

### Installation

You have to install Java.

Download Hadoop. Unpack with tar xzf hadoop-x.y.z.tar.gz.

Add to .bashrc.

export HADOOP_HOME=/path/to/your/hadoop!!
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export
PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

Use command to refresh the script: source ~/.bashrc

In hadoop-env.sh change export JAVA_HOME=... to /where/you/installed/java/jdkx.y.z_version.

In the following files add the following between <configuration> elements:

core-site.xml
<property> 
  <name>fs.default.name</name> 
  <value>hdfs://localhost:9000</value> 
</property>
      	
hdfs-site.xml
<property> 
  <name>dfs.replication</name> 
  <value>1</value> 
</property> 
<property> 
  <name>dfs.name.dir</name> 
  <value>file:///path/to/your/hadoop/hadoopinfra/hdfs/namenode </value> !!
</property> 
<property> 
  <name>dfs.data.dir</name>
  <value>file:///path/to/your/hadoop/hadoopinfra/hdfs/datanode </value> !!
</property>

yarn-site.xml
<property> 
  <name>yarn.nodemanager.aux-services</name> 
  <value>mapreduce_shuffle</value> 
</property>
 
mapred-site.xml should be renamed with the cp command.

cp mapred-site.xml.template mapred-site.xml

<property> 
  <name>mapreduce.framework.name</name> 
  <value>yarn</value> 
</property>

### Test installation

hdfs namenode -format

Good output:
...
... INFO util.ExitUtil: Exiting with status 0
...

start-dfs.sh (and shutdown with stop-dfs.sh)

Good output:
Starting namenodes on [localhost] 
...
Starting secondary namenodes [0.0.0.0]
...

If the connection is refused add:
# My addition to solve the refused connection problem when running start-dfs.sh
export HADOOP_SSH_OPTS="-p 22"
... to the .../hadoop/etc/hadoop/hadoop-env.sh and install openssh with sudo apt-get install openssh-server.

start-yarn.sh

Good output:
...
localhost: starting nodemanager, logging to ...

Access Hadoop and clusters:
http://localhost:50070/
http://localhost:8088/