## Apache Spark

Apache Spark allows a computation to be performed across many different computers at the same time. It
abstracts a dataset by creating a Resilient Distributed Dataset (RDD) and allows the user to apply functions
and transformations on it. It also abstract the computer cluster through the Spark Context.